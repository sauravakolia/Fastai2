# -*- coding: utf-8 -*-
"""minibatch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BJwNFiS1TFWoZJfVMX6aIWjLxy0r_t-y
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

import torch.nn.functional as F

import operator

def test(a,b,cmp,cname=None):
    if cname is None: cname=cmp.__name__
    assert cmp(a,b),f"{cname}:\n{a}\n{b}"

def test_eq(a,b): test(a,b,operator.eq,'==')

#export
def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)
def test_near(a,b): test(a,b,near)

#export
from pathlib import Path
from IPython.core.debugger import set_trace
from fastai import datasets
import pickle, gzip, math, torch, matplotlib as mpl
import matplotlib.pyplot as plt
from torch import tensor

MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'

#export
def get_data():
    path = datasets.download_data(MNIST_URL, ext='.gz')
    with gzip.open(path, 'rb') as f:
        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
    return map(tensor, (x_train,y_train,x_valid,y_valid))

def normalize(x, m, s): return (x-m)/s

mpl.rcParams['image.cmap'] = 'gray'

x_train,y_train,x_valid,y_valid = get_data()
# train_mean,train_std = x_train.mean(),x_train.std()
# x_train = normalize(x_train, train_mean, train_std)
# x_valid = normalize(x_valid, train_mean, train_std)

n,m = x_train.shape
c = y_train.max()+1
nh = 50

from torch import nn

class Model(nn.Module):
    def __init__(self, n_in, nh, n_out):
        super().__init__()
        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]
        
    def __call__(self, x):
        for l in self.layers: x = l(x)
        return x

model = Model(m, nh, 10)

pred=model(x_train)

"""### Cross entropy loss"""

def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdims=True))).log()

sm_pred=log_softmax(pred)

y_train[:3]

"""The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:

$$ -\sum x\, \log p(x) $$

But since our $x$s are 1-hot encoded, this can be rewritten as $-\log(p_{i})$ where i is the index of the desired target.
"""

sm_pred[[0,1,2], [5,0,4]]

def nll(input,target): return -input[range(target.shape[0]),target].mean()

# def nll(input, target): return -input[range(target.shape[0]), target].mean()

range(y_train.shape[0])
y_train

loss = nll(sm_pred, y_train)

loss

"""Note that the formula 

$$\log \left ( \frac{a}{b} \right ) = \log(a) - \log(b)$$ 

gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`
"""

def log_softmax(x):return x-(x.exp().sum(-1,keepdims=True).log())
# def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()

sm_pred=log_softmax(pred)

test_near(nll(log_softmax(pred), y_train), loss)

"""Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:

$$\log \left ( \sum_{j=1}^{n} e^{x_{j}} \right ) = \log \left ( e^{a} \sum_{j=1}^{n} e^{x_{j}-a} \right ) = a + \log \left ( \sum_{j=1}^{n} e^{x_{j}-a} \right )$$

where a is the maximum of the $x_{j}$.
"""

def logsumexp(x):
  m = x.max(-1)[0] 
  return m+(x-m[:,None]).exp().sum(-1).log()

test_near(logsumexp(pred), pred.logsumexp(-1))

def log_softmax(x): return x-x.logsumexp(-1,keepdim=True)

test_near(nll(log_softmax(pred), y_train), loss)

test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)

test_near(F.cross_entropy(pred, y_train), loss)

"""# Basic training loop"""

loss_func=F.cross_entropy

# def accuracy(out,yb):return (torch.argmax(out,dim=1) ==yb).float().mean()

def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()

bs=64                  # batch size

xb = x_train[0:bs]     # a mini-batch from x
preds = model(xb)      # predictions
preds[0], preds.shape

preds[0]

yb=y_train[0:bs]
accuracy(preds,yb)

loss_func(preds,yb)

lr=0.5
epochs=1

# for epoch in range(epochs):
#   for i in range(n-1//bs):
#     start_i=i*bs
#     end_i=i*bs+bs
#     xb = x_train[start_i:end_i]
#     yb=y_train[start_i:end_i]
#     preds=model(xb)

#     loss=loss_func(preds,yb)

#     loss.backward()
#     with torch.no_grad():
#       for l in model.layers:
#         if hasattr(l,'weight'):
#           l.weight-=l.weight.grad *lr
#           l.bias-=lr*l.bias.grad
#           l.weight.grad.zero_()
#           l.bias.grad.zero_()  

for epoch in range(epochs):
    for i in range((n-1)//bs + 1):
#         set_trace()
        start_i = i*bs
        end_i = start_i+bs
        xb = x_train[start_i:end_i]
        yb = y_train[start_i:end_i]
        loss = loss_func(model(xb), yb)

        loss.backward()
        with torch.no_grad():
            for l in model.layers:
                if hasattr(l, 'weight'):
                    l.weight -= l.weight.grad * lr
                    l.bias   -= l.bias.grad   * lr
                    l.weight.grad.zero_()
                    l.bias  .grad.zero_()

loss_func(model(xb), yb), accuracy(model(xb), yb)

"""## Using parameters and optim

### Parameters
"""

class Model(nn.Module):
  def __init__(self,n_in,nh,n_out):
    super().__init__()
    self.l1=nn.Linear(n_in,nh)
    self.l2=nn.Linear(nh,n_out)

  # def __call__(self,x):
  #   return self.l2(F.ReLU(self.l1(x)))

  def __call__(self, x): return self.l2(F.relu(self.l1(x)))

model=Model(m,nh,10)

model

model.l1

def fit():
  for epoch in range(epochs):
    for i in range((n-1)//bs +1):
      start_i=i*bs
      end_i=start_i+bs
      xb=x_train[start_i:end_i]
      yb=y_train[start_i:end_i]
      pred=model(xb)
      loss=loss_func(pred,yb)

      loss.backward()
      with torch.no_grad():
        for p in model.parameters():
          p-=lr*p.grad
          model.zero_grad()

          # if hasattr(l,'weight'):
          #   l.weight-=lr*l.weight.grad
          #   l.bias-=lr*l.bias.grad

          #   l.weight.grad.zero_()
          #   l.bias.grad.zero_()

fit()

loss_func(model(xb), yb), accuracy(model(xb), yb)

class DummyModule():
  def __init__(self,n_in,nh,n_out):
    self._modules={}
    self.l1=nn.Linear(n_in,nh)
    self.l2=nn.Linear(nh,n_out)

  def __setattr__(self,k,v):
    if not k.startswith("_"):self._modules[k]=v
    super().__setattr__(k,v)

  def __repr__(self):return f'{self._modules}'

  def parameters(self):
    for l in self._modules.values():
      for p in l.parameters(): yield p

mdl=DummyModule(m,nh,10)

mdl

[o.shape for o in mdl.parameters()]

"""### Registering modules

We can use the original `layers` approach, but we have to register the modules.
"""

layers=[nn.Linear(m,nh),nn.ReLU(),nn.Linear(nh,10)]

class Model(nn.Module):
  def __init__(self,layers):
    super().__init__()
    self.layers=layers
    for i,l in enumerate(self.layers):self.add_module(f'layer_{i}',l)

  def __call__(self,x):
    for l in self.layers: l=l(x)  
    return x

model = Model(layers)
model

"""### nn.ModuleList"""

class SequentialModel(nn.Module):
  def __init__(self,layers):
    super().__init__()
    self.layers=nn.ModuleList(layers)

  def __call__(self,x):
    for l in self.layers:  x=l(x)
    return x

model=SequentialModel(layers)

model

fit()
loss_func(model(xb), yb), accuracy(model(xb), yb)

"""### nn.Sequential"""

model=nn.Sequential(nn.Linear(m,nh),nn.ReLU(),nn.Linear(nh,10))

fit()
loss_func(model(xb), yb), accuracy(model(xb), yb)



model

"""### optim

Let's replace our previous manually coded optimization step:

```python
with torch.no_grad():
    for p in model.parameters(): p -= p.grad * lr
    model.zero_grad()
```

and instead use just:

```python
opt.step()
opt.zero_grad()
```
"""

class optimizer():
  def __init__(self,params,lr=0.5):
    self.params,self.lr=list(params),lr

  def one_step(self):  
    with torch.no_grad():
      for p in self.params:
        p-=self.lr*p.grad

  def zero_grad(self):
    for p in self.params:
      p.grad.data.zero_()

model=nn.Sequential(nn.Linear(m,nh),nn.ReLU(),nn.Linear(nh,10))

opt=optimizer(model.parameters())

for epoch in range(epochs):
  for i in range((n-1)//bs +1):
    start_i=i*bs
    end_i=start_i+bs
    xb=x_train[start_i:end_i]
    yb=y_train[start_i:end_i]
    preds=model(xb)
    loss=loss_func(preds,yb)

    loss.backward()
    opt.one_step()
    opt.zero_grad()

loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
loss,acc

"""PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"""

from torch import optim

def get_model():
  model=nn.Sequential(nn.Linear(m,nh),nn.ReLU(),nn.Linear(nh,10))
  return model,optim.SGD(model.parameters(),lr=lr)

model,opt=get_model()

loss_func(model(xb), yb)

for epoch in range(epochs):
  for i in range((n-1)//bs +1):
    start_i=i*bs
    end_i=start_i+bs
    xb=x_train[start_i:end_i]
    yb=y_train[start_i:end_i]
    preds=model(xb)
    loss=loss_func(preds,yb)

    loss.backward()
    opt.step()
    opt.zero_grad()

loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
loss,acc

"""## Dataset and DataLoader

### Dataset

It's clunky to iterate through minibatches of x and y values separately:

```python
    xb = x_train[start_i:end_i]
    yb = y_train[start_i:end_i]
```

Instead, let's do these two steps together, by introducing a `Dataset` class:

```python
    xb,yb = train_ds[i*bs : i*bs+bs]
```
"""

class Dataset():
  def __init__(self,x,y):
    self.x=x
    self.y=y

  def __len__(self):
    return len(self.x) 

  def __getitem__(self,i):
    return self.x[i],self.y[i]

train_ds,valid_ds=Dataset(x_train,y_train),Dataset(x_valid,y_valid)
assert len(train_ds)==len(x_train)
assert len(valid_ds)==len(x_valid)

model,opt=get_model()

for epoch in range(epochs):
  for i in range((n-1)//bs +1):
    start_i=i*bs
    end_i=start_i+bs
    xb,yb=train_ds[start_i:end_i]
    
    preds=model(xb)
    loss=loss_func(preds,yb)

    loss.backward()
    opt.step()
    opt.zero_grad()

loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
assert acc>0.7
loss,acc

"""### DataLoader

Previously, our loop iterated over batches (xb, yb) like this:

```python
for i in range((n-1)//bs + 1):
    xb,yb = train_ds[i*bs : i*bs+bs]
    ...
```

Let's make our loop much cleaner, using a data loader:

```python
for xb,yb in train_dl:
    ...
```
"""

class DataLoader():
  def __init__(self,ds,bs):
    self.ds=ds
    self.bs=bs

  def __iter__(self): 
    for i in range(0,len(self.ds),self.bs): yield self.ds[i:i+self.bs]

train_dl,valid_dl=DataLoader(train_ds,bs),DataLoader(valid_ds,bs)

xb,yb = next(iter(valid_dl))
assert xb.shape==(bs,28*28)
assert yb.shape==(bs,)

def fit():
  for epoch in range(epochs):  
    for xb,yb in train_dl: 
      preds=model(xb)
      loss=loss_func(preds,yb)

      loss.backward()
      opt.step()
      opt.zero_grad()

fit()

loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
assert acc>0.7
loss,acc

"""### Random sampling

We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized.
"""

class Sampler():
  def __init__(self,ds,bs,shuffle=False):
    self.n=len(ds)
    self.bs=bs
    self.shuffle=shuffle
  
  def __iter__(self):
    self.idxs=torch.randperm(self.n)if self.shuffle  else torch.arange(self.n)
    for i in range(0,self.n,self.bs):yield self.idxs[i:i+self.bs]

small_ds = Dataset(*train_ds[:10])

s = Sampler(small_ds,3,False)
[o for o in s]

s = Sampler(small_ds,3,True)
[o for o in s]

def collate(b):
    xs,ys = zip(*b)
    return torch.stack(xs),torch.stack(ys)

class DataLoader():
  def __init__(self,ds,sampler,collate_fn=collate):
    self.ds=ds
    self.sampler=sampler
    self.collate_fn=collate_fn

  def __iter__(self):
    for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])

train_samp=Sampler(train_ds,bs,shuffle=True)
valid_samp=Sampler(valid_ds,bs,shuffle=True)

train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)
valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)

fit()

loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
assert acc>0.7
loss,acc

"""### PyTorch DataLoader"""

from torch.utils.data import DataLoader,RandomSampler,SequentialSampler

train_dl=DataLoader(train_ds,bs,sampler=RandomSampler(train_ds),collate_fn=collate)
valid_dl=DataLoader(valid_ds,bs,sampler=RandomSampler(valid_ds),collate_fn=collate)

fit()
loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
assert acc>0.7
loss,acc

"""PyTorch's defaults work fine for most things however:"""

train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)
valid_dl = DataLoader(valid_ds, bs, shuffle=False)

fit()
loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
assert acc>0.7
loss,acc

"""## Validation"""

# def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
#    for epoch in range(epochs):
#      model.train()  
#      for xb,yb in train_dl: 
#       preds=model(xb)
#       loss=loss_func(preds,yb)

#       loss.backward()
#       opt.step()
#       opt.zero_grad()

#      model.eval()

#      with torch.no_grad():
#        tot_loss,tot_acc = 0.,0.
#        for epoch in range(epochs):
#          for xb,yb in train_dl: 
#            preds=model(xb)
#            tot_loss+=loss_func(preds,yb)
#            tot_acc+=accuracy(preds,yb)

#        nv=len(valid_dl)
#        print(epoch, tot_loss/nv, tot_acc/nv)
#     return tot_loss/nv, tot_acc/nv
#     # for epoch in range(epochs):
#     # return tot_loss/nv, tot_acc/nv

def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
  for epoch in range(epochs):
    model.train()  
    for xb,yb in train_dl:
      preds=model(xb)
      loss=loss_func(preds,yb)

      loss.backward()
      opt.step()
      opt.zero_grad()
    model.eval()
    with torch.no_grad():
      tot_loss,tot_acc = 0.,0.
      for xb,yb in valid_dl:
        pred = model(xb)
        tot_loss += loss_func(pred, yb)
        tot_acc  += accuracy (pred,yb)
    nv = len(valid_dl)
    print(epoch, tot_loss/nv, tot_acc/nv)
  return tot_loss/nv, tot_acc/nv

"""`get_dls` returns dataloaders for the training and validation sets:"""

def get_dls(train_ds,valid_ds,bs,**kwargs):
  return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
            DataLoader(valid_ds, batch_size=bs*2, **kwargs))

train_dl,valid_dl=get_dls(train_ds,valid_ds,bs)
model,opt=get_model()
loss,acc=fit(5, model, loss_func, opt, train_dl, valid_dl)

assert acc>0.9